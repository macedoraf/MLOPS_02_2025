{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_bguvWHyu4U"
   },
   "source": [
    "# Identificação\n",
    "\n",
    "**Assunto:** Modelagem\n",
    "\n",
    "**Tutor:** Manoel Veríssimo dos Santos Neto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gfevqw7Eyf76"
   },
   "source": [
    "# Processamento de Linguagem Natural utilizando a biblioteca Hugging Face Transformers\n",
    "\n",
    "## 1- Objetivos de Aprendizagem\n",
    "\n",
    "Neste Notebook, vamos explorar exemplos reais de soluções para tarefas de Processamento de Linguagem Natural (NLP) utilizando a biblioteca Hugging Face Transformers. Nosso primeiro exemplo será o *fine-tuning* do modelo BERT para a análise de sentimentos em português. Em seguida, apresentaremos dois exemplos utilizando apenas modelos pré-treinados com os *pipelines* da Hugging Face. O segundo exemplo demonstrará o uso de* Named Entity Recognition* (NER), e o terceiro exemplo abordará a tradução automática. Estes exemplos práticos fornecerão uma visão abrangente de como aplicar técnicas avançadas de NLP para resolver problemas do mundo real.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJA5knmZwYFH"
   },
   "source": [
    "# 2- *Fine-Tuning* do BERT para Análise de Sentimentos\n",
    "\n",
    "Neste tópico, vamos realizar o *fine-tunin*g do modelo BERT para a tarefa de análise de sentimentos utilizando a biblioteca Hugging Face Transformers. Vamos usar um *dataset* de comentários sobre compras em português.\n",
    "\n",
    "**Importante:** Para o treinamento deve ser usada a rumtime com **GPU**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZW11YlHyzkX6"
   },
   "source": [
    "## 2.1- Instalação das Dependências\n",
    "\n",
    "Vamos começar instalando as bibliotecas necessárias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hvZsiZ_9xRbd"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install torch\n",
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cyIq7wB3uRTl"
   },
   "source": [
    "## 2.2- Configuração de variáveis globais\n",
    "\n",
    "Vamos configurar as variáveis de parâmetros para serem utilizadas durante o código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ee6ne-tIuQj6"
   },
   "outputs": [],
   "source": [
    "model_id = \"adalbertojunior/distilbert-portuguese-cased\"\n",
    "max_length= 512\n",
    "num_labels = 3\n",
    "batch_size = 28\n",
    "results_path = \"./results\"\n",
    "pretrained_path = \"./sentiment-analysis-bert-portuguese\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TO9J2RgG0Wga"
   },
   "source": [
    "## 2.3- Importação das Bibliotecas\n",
    "\n",
    "Vamos importar as bibliotecas necessárias para carregar o *dataset*, tokenizar os textos, configurar o modelo BERT, e realizar o treinamento e avaliação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r4NW3Bfk0Rwg"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import evaluate\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "import wandb\n",
    "\n",
    "wandb.init(mode=\"disabled\")  # Desabilita o Wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-j83Sjz0h-a"
   },
   "source": [
    "## 2.4- Carregar o *Dataset*\n",
    "\n",
    "Vamos carregar um *dataset* de comentários de pedidos em português.\n",
    "Para este exemplo, vamos utilizar o *dataset* `verissimomanoel/olist_customers_review` da Hugging Face, que contém comentários rotulados para análise de sentimentos.\n",
    "No dataset já tem uma parte de treino `train` e outra de para teste `test`. Contudo precisamos dividir o treino mais uma vez para ter uma parte para validação `val` que será usada durante o treino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L3ue_4LH0eVw"
   },
   "outputs": [],
   "source": [
    "# Carrega o dataset que tem train e test\n",
    "dataset = load_dataset(\"verissimomanoel/olist_customers_review\", trust_remote_code=True)\n",
    "\n",
    "# Divide o treino em 80% e 20%, sendo os 80 para treino e os 20 para validação\n",
    "ds_train_split = dataset[\"train\"].train_test_split(test_size=0.2)\n",
    "\n",
    "# Monta o dataset com todas as partes train, test e val\n",
    "dataset = DatasetDict({\n",
    "    \"train\": ds_train_split[\"train\"],\n",
    "    \"test\": dataset[\"test\"],\n",
    "    \"val\": ds_train_split[\"test\"],\n",
    "})\n",
    "\n",
    "# Separa as partes do dataset\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "val_dataset = dataset['val']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BYfC-Ebb0tLc"
   },
   "source": [
    "## 2.5- Visualização do *Dataset*\n",
    "\n",
    "Vamos visualizar a estrutura do *dataset* carregado. Para tal, as sequências de código [6] e [7] abaixo mostrarão, na forma de gráfico, os conjuntos de dados para treino, teste e validação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eWJaKskM0oIc"
   },
   "outputs": [],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6-2MGVJct4Oo"
   },
   "outputs": [],
   "source": [
    "def show_info_dataset(dataset, title):\n",
    "    # Converter o dataset para um DataFrame do pandas\n",
    "    df = dataset.to_pandas()\n",
    "\n",
    "    # Contar as ocorrências na coluna 'label'\n",
    "    label_counts = df['label'].value_counts()\n",
    "\n",
    "    # Mapeamento dos labels para nomes\n",
    "    label_names = {0: 'Negativo', 1: 'Positivo', 2: 'Neutro'}\n",
    "\n",
    "    # Obter os nomes das labels\n",
    "    labels = [label_names[label] for label in label_counts.index]\n",
    "\n",
    "    # Definir as cores para cada label\n",
    "    colors = ['green', 'red', 'blue']\n",
    "\n",
    "    # Plotar o gráfico de barras\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(labels, label_counts, color=colors)\n",
    "\n",
    "    # Adicionar os totais em cima das barras\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, yval, int(yval), va='bottom')  # va: vertical alignment\n",
    "\n",
    "    # Configurar o título e os rótulos dos eixos\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Label')\n",
    "    plt.ylabel('Total')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R2C-42X2t5BH"
   },
   "outputs": [],
   "source": [
    "show_info_dataset(train_dataset, 'Distribuição por Classe - Treino')\n",
    "show_info_dataset(val_dataset, 'Distribuição por Classe - Validação')\n",
    "show_info_dataset(test_dataset, 'Distribuição por Classe - Teste')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MozQLcDL01Ir"
   },
   "source": [
    "## 2.6- Tokenização do *Dataset*\n",
    "\n",
    "Vamos carregar o tokenizer do BERT e usá-lo para tokenizar os textos no *dataset.* Vamos usar o modelo `neuralmind/bert-base-portuguese-cased`, que é um BERT treinado em português.\n",
    "\n",
    "O comando train_dataset.shuffle().select(range(5000)) é responsaável pelo\n",
    "embaralhamento e seleção de amostras. Neste caso está selecionando as primeiras 5000 amostras do conjunto de dados embaralhado.\n",
    "\n",
    "O comando train_dataset.map(tokenize_function, batched=True) realiza a tokenização no conjunto de dados de treinamento (train_dataset) e usa a função map() para aplicar a função de tokenização a cada lote de amostras do conjunto de dados. Com batched=True, a função é aplicada em lotes de amostrase não em uma amostra por vez, isso torna o processo mais eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lv0J4FuF0r19"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load config for 'adalbertojunior/distilbert-portuguese-cased'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'adalbertojunior/distilbert-portuguese-cased' is the correct path to a directory containing a config.json file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMissingSchema\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/configuration_utils.py:594\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    593\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 594\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/file_utils.py:1921\u001b[0m, in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_remote_url(url_or_filename):\n\u001b[1;32m   1920\u001b[0m     \u001b[38;5;66;03m# URL, so get it from the cache (downloading if necessary)\u001b[39;00m\n\u001b[0;32m-> 1921\u001b[0m     output_path \u001b[38;5;241m=\u001b[39m \u001b[43mget_from_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1922\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_or_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1927\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1928\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1929\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1930\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1931\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(url_or_filename):\n\u001b[1;32m   1932\u001b[0m     \u001b[38;5;66;03m# File, and it exists.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/file_utils.py:2217\u001b[0m, in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   2215\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in cache or force_download set to True, downloading to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemp_file\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2217\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2219\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstoring \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcache_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/file_utils.py:2062\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers)\u001b[0m\n\u001b[1;32m   2061\u001b[0m     headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRange\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbytes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresume_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 2062\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2063\u001b[0m _raise_for_status(r)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/requests/sessions.py:575\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    563\u001b[0m req \u001b[38;5;241m=\u001b[39m Request(\n\u001b[1;32m    564\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod\u001b[38;5;241m.\u001b[39mupper(),\n\u001b[1;32m    565\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    573\u001b[0m     hooks\u001b[38;5;241m=\u001b[39mhooks,\n\u001b[1;32m    574\u001b[0m )\n\u001b[0;32m--> 575\u001b[0m prep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m proxies \u001b[38;5;241m=\u001b[39m proxies \u001b[38;5;129;01mor\u001b[39;00m {}\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/requests/sessions.py:484\u001b[0m, in \u001b[0;36mSession.prepare_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    483\u001b[0m p \u001b[38;5;241m=\u001b[39m PreparedRequest()\n\u001b[0;32m--> 484\u001b[0m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmerge_setting\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdict_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCaseInsensitiveDict\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmerge_setting\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmerge_setting\u001b[49m\u001b[43m(\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcookies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmerged_cookies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhooks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmerge_hooks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhooks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m p\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/requests/models.py:367\u001b[0m, in \u001b[0;36mPreparedRequest.prepare\u001b[0;34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_method(method)\n\u001b[0;32m--> 367\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_headers(headers)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/requests/models.py:438\u001b[0m, in \u001b[0;36mPreparedRequest.prepare_url\u001b[0;34m(self, url, params)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m scheme:\n\u001b[0;32m--> 438\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MissingSchema(\n\u001b[1;32m    439\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid URL \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m: No scheme supplied. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    440\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerhaps you meant https://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    441\u001b[0m     )\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m host:\n",
      "\u001b[0;31mMissingSchema\u001b[0m: Invalid URL '/api/resolve-cache/models/adalbertojunior/distilbert-portuguese-cased/517a1012c1b7a36b1660ecac4ec27b6c294a5077/config.json?%2Fadalbertojunior%2Fdistilbert-portuguese-cased%2Fresolve%2Fmain%2Fconfig.json=&etag=%22623f62c55c6ddaea0f07c9540c3e23162d82c929%22': No scheme supplied. Perhaps you meant https:///api/resolve-cache/models/adalbertojunior/distilbert-portuguese-cased/517a1012c1b7a36b1660ecac4ec27b6c294a5077/config.json?%2Fadalbertojunior%2Fdistilbert-portuguese-cased%2Fresolve%2Fmain%2Fconfig.json=&etag=%22623f62c55c6ddaea0f07c9540c3e23162d82c929%22?",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForPreTraining  \u001b[38;5;66;03m# Or BertForPreTraining for loading pretraining heads\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModel \u001b[38;5;66;03m# Or\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForPreTraining\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43madalbertojunior/distilbert-portuguese-cased\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madalbertojunior/distilbert-portuguese-cased\u001b[39m\u001b[38;5;124m'\u001b[39m, do_lower_case\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtokenize_function\u001b[39m(examples):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:424\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    422\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_auto\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[0;32m--> 424\u001b[0m     config, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map:\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trust_remote_code:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:637\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    635\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_or_path\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pretrained_model_name_or_path\n\u001b[1;32m    636\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 637\u001b[0m config_dict, _ \u001b[38;5;241m=\u001b[39m \u001b[43mPretrainedConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trust_remote_code:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/configuration_utils.py:546\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    544\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    545\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 546\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;66;03m# That config file may point us toward another config file to use.\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfiguration_files\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/configuration_utils.py:630\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    624\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe couldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt connect to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to load this model and it looks like \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    625\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not the path to a directory conaining a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfiguration_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    626\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCheckout your internet connection or see how to run the library in offline mode at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    627\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/installation#offline-mode\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    628\u001b[0m     )\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[0;32m--> 630\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load config for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfiguration_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m file\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    635\u001b[0m     )\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;66;03m# Load config dict\u001b[39;00m\n\u001b[1;32m    639\u001b[0m     config_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_dict_from_json_file(resolved_config_file)\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load config for 'adalbertojunior/distilbert-portuguese-cased'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'adalbertojunior/distilbert-portuguese-cased' is the correct path to a directory containing a config.json file"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(model_id)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=max_length)\n",
    "\n",
    "# Reduz o tamanho dos datasets apenas para conseguir rodar no Google Colab por menos tempo, para rodar com o dataset completo só comentar as próximas 3 linhas\n",
    "train_dataset = train_dataset.shuffle().select(range(5000))\n",
    "test_dataset = test_dataset.shuffle().select(range(1000))\n",
    "val_dataset = test_dataset.shuffle().select(range(800))\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "II6ToPVi6L3r"
   },
   "source": [
    "## 2.7- Formatando o Dataset\n",
    "\n",
    "Vamos definir o formato dos datasets para que o *Trainer* da Hugging Face possa processá-los corretamente.\n",
    "\n",
    "*   **set_format(type='torch', columns=['input_ids', 'attention_mask', 'label']):** Este método converte os datasets para o formato do **PyTorch** (tensors) para que possam ser usados diretamente em redes neurais. A função especifica que apenas as colunas input_ids, attention_mask e label serão mantidas no formato final.\n",
    "\n",
    "  *   **input_ids:** Contém os identificadores numéricos que representam as palavras ou tokens da entrada.\n",
    "\n",
    "  *   **attention_mask:** Indica quais tokens são relevantes (1) e quais são padding (0), para que o modelo saiba onde prestar atenção.\n",
    "\n",
    "  *   **label:** São as etiquetas associadas a cada exemplo, que o modelo deve prever (por exemplo, para uma tarefa de classificação)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vl1_Y6kc6NBB"
   },
   "outputs": [],
   "source": [
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h84dMysP07gu"
   },
   "source": [
    "## 2.8- Configuração do Modelo BERT\n",
    "\n",
    "Vamos configurar o modelo BERT para a tarefa de classificação de sequência. Neste caso, estamos utilizando a versão `neuralmind/bert-base-portuguese-cased` do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BvrAz-oJ0zfI"
   },
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(model_id, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dETjnEzR1D3z"
   },
   "source": [
    "## 2.9- Função de Avaliação\n",
    "\n",
    "Vamos definir uma função de avaliação para calcular a precisão do modelo durante a avaliação. Utilizaremos a métrica de precisão (`accuracy`) fornecida pela biblioteca `datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ENyx3BvE1BK-"
   },
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    # Transforma os logits (saída do modelo) em previsões de classe, atribuindo a cada exemplo a classe com a\n",
    "    # maior probabilidade, ou seja, a que tem o maior valor de logit.\n",
    "    predictions = torch.argmax(torch.tensor(logits), dim=-1)\n",
    "\n",
    "    return metric.compute(predictions=predictions, references=torch.tensor(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ptvexshA14Jq"
   },
   "source": [
    "## 2.10- Configuração dos Argumentos de Treinamento\n",
    "\n",
    "Vamos definir os parâmetros de treinamento, incluindo a taxa de aprendizado, tamanho do batch, número de épocas, e a estratégia de avaliação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6-eN47ng1GQw"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=results_path,                  # Diretório de saída para os resultados\n",
    "    eval_strategy=\"epoch\",                    # Estratégia de avaliação (avaliar a cada época)\n",
    "    learning_rate=3e-5,                       # Taxa de aprendizado\n",
    "    per_device_train_batch_size=batch_size,   # Tamanho do batch de treino\n",
    "    per_device_eval_batch_size=batch_size,    # Tamanho do batch de avaliação\n",
    "    num_train_epochs=3,                       # Número de épocas de treinamento\n",
    "    weight_decay=0.01,                        # Decaimento de peso\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKnYBv_y18vC"
   },
   "source": [
    "## 2.11- Treinamento do Modelo\n",
    "\n",
    "Vamos criar um objeto `Trainer` com o modelo, dados de treino e validação, e os argumentos de treinamento definidos nos comando acima. Em seguida, vamos iniciar o treinamento do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3u98LZ_T16Ql"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVZP3Vs_7zlI"
   },
   "source": [
    "## 2.12- Avaliação do Modelo - *Evaluate*\n",
    "\n",
    "Vamos avaliar o modelo no conjunto de teste ( com o comando trainer.evaluate) e exibir a precisão com a função show_info logo abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pGMs6gqg2iPU"
   },
   "outputs": [],
   "source": [
    "results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "print(f\"Acurácia no conjunto de teste: {results['eval_accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mo6dKps6umDv"
   },
   "source": [
    "## 2.13- Avaliação do Modelo - *Predict*\n",
    "\n",
    "Vamos avaliar o modelo no conjunto de teste e exibir a matriz de confusão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3e5RvhcWun1A"
   },
   "outputs": [],
   "source": [
    "raw_pred, _, _ = trainer.predict(test_dataset=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r59x4Cggupsz"
   },
   "outputs": [],
   "source": [
    "def show_info(y_true, y_pred, title='Confusion matrix', cmap='Blues'):\n",
    "    target_names = ['Negativo', 'Positivo', 'Neutro']\n",
    "    print(classification_report(y_true, y_pred, target_names=target_names))\n",
    "\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    disp.plot(ax=ax, xticks_rotation='vertical', cmap=plt.cm.Blues,values_format='g')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3-7yNnLXuqxx"
   },
   "outputs": [],
   "source": [
    "# Pré-processar previsões brutas\n",
    "y_pred = np.argmax(raw_pred, axis=1)\n",
    "y_true = test_dataset[\"label\"]\n",
    "\n",
    "show_info(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zbnha-EL78sr"
   },
   "source": [
    "## 2.14- Salvar o Modelo\n",
    "\n",
    "Vamos salvar o modelo treinado e o tokenizer para uso futuro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Treod7WH1EWh"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(pretrained_path)\n",
    "tokenizer.save_pretrained(pretrained_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fe0UgAUe8GlQ"
   },
   "source": [
    "## 2.15- Predição de Novos Exemplos\n",
    "\n",
    "Vamos definir uma função para prever o sentimento de novas frases usando o modelo treinado. Em seguida, vamos testar o modelo com uma nova frase e exibir o resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AmHP2jYB8DcB"
   },
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    # Certificar que o modelo e os inputs estão no mesmo dispositivo (CPU ou GPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Tokeniza o texto, aplica padding e truncamento, converte para tensor PyTorch e move os dados para o dispositivo especificado.\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Executa o modelo com as entradas fornecidas, passando os tensores de input como argumentos para gerar as previsões ou saídas do modelo.\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Aplica a função softmax as saídas do modelo para converter os valores em probabilidades, normalizadas ao longo da última dimensão.\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "    # Retorna a maior probabilidade\n",
    "    return probs.argmax().item()\n",
    "\n",
    "example_text = \"Eu adorei esse filme! Foi fantástico.\"\n",
    "predicted_label = predict_sentiment(example_text)\n",
    "sentiment = ['Negativo', 'Positivo', 'Neutro']\n",
    "print(f\"Sentimento previsto: {sentiment[predicted_label]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HzLgLltmu5Gc"
   },
   "source": [
    "## 2.16- F1 *Score* - Importância em Datasets Desbalanceados\n",
    "\n",
    "O F1 *Score* é uma métrica usada para avaliar a performance de um modelo de classificação, especialmente quando lidamos com datasets desbalanceados. Para entender o F1 Score, é importante conhecer alguns conceitos básicos:\n",
    "\n",
    "1. **Acurácia (*Accuracy*)**: Métrica de avaliação utilizada para medir a proporção de previsões corretas em relação ao total de previsões feitas por um modelo de classificação. Ela é definida pela fórmula:\n",
    "    $$\n",
    "    \\text{Acurácia} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "    $$\n",
    "\n",
    "Onde:\n",
    "- **TP** (*True Positives*): Verdadeiros Positivos\n",
    "- **TN** (*True Negatives*): Verdadeiros Negativos\n",
    "- **FP** (*False Positives*): Falsos Positivos\n",
    "- **FN** (*False Negatives*): Falsos Negativos\n",
    "\n",
    "2. **Precisão (*Precision*)**: É a proporção de verdadeiros positivos (TP) entre todas as instâncias que o modelo previu como positivas. Em outras palavras, é a quantidade de previsões corretas de uma classe específica em relação ao total de previsões feitas para essa classe.\n",
    "   $$\n",
    "   \\text{Precisão} = \\frac{TP}{TP + FP}\n",
    "   $$\n",
    "   onde FP são os falsos positivos.\n",
    "\n",
    "3. **Revocação (*Recall*)**: É a proporção de verdadeiros positivos entre todas as instâncias que são realmente positivas. Ou seja, é a quantidade de previsões corretas de uma classe específica em relação ao total de instâncias reais dessa classe.\n",
    "   $$\n",
    "   \\text{Revocação} = \\frac{TP}{TP + FN}\n",
    "   $$\n",
    "   onde FN são os falsos negativos.\n",
    "\n",
    "4. **F1 Score**: É a média harmônica entre a Precisão e a Revocação. A média harmônica é usada aqui porque penaliza valores extremos, garantindo que o F1* Score* será baixo se um dos dois (Precisão ou Revocação) estiver baixo.\n",
    "   $$\n",
    "   \\text{F1 Score} = 2 \\times \\frac{\\text{Precisão} \\times \\text{Revocação}}{\\text{Precisão} + \\text{Revocação}}\n",
    "   $$\n",
    "\n",
    "### Por que usar o F1 Score em datasets desbalanceados?\n",
    "\n",
    "Em datasets desbalanceados, onde uma classe é muito mais frequente do que outra, métricas como a acurácia podem ser enganosas. Por exemplo, se temos 95% das instâncias de uma classe e apenas 5% de outra, um modelo que sempre prevê a classe majoritária terá alta acurácia, mas não será útil para detectar a classe minoritária.\n",
    "\n",
    "O F1 Score é importante porque leva em consideração tanto a Precisão quanto a Revocação. Em um cenário desbalanceado, isso ajuda a fornecer uma visão mais equilibrada da performance do modelo, destacando se ele é capaz de identificar a classe minoritária com precisão e frequência suficientes.\n",
    "\n",
    "Assim, o F1 Score é particularmente útil quando a prioridade é garantir que tanto a taxa de detecção dos positivos (revocação) quanto a qualidade das detecções positivas (precisão) são importantes, o que é frequentemente o caso em situações desbalanceadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cmgLgoLyzKLA"
   },
   "source": [
    "## 2.17- Exercícios *Fine-Tuning*\n",
    "\n",
    "\n",
    "1.   Alterar alguns parâmetros de treinamento, como: batch_size, learning rate e número de épocas. Avaliar qual o impacto negativo ou positivo na alteração desses parâmetros.\n",
    "2.   Utilizar alguma técnica de balanceamento de *dataset* e avaliar os resultados, Ex.: *Oversampling* e *Undersampling*\n",
    "\n",
    "**Importante:**\n",
    "\n",
    "*   Todas as alterações devem ser registradas no Wandb para que seja possível realizar comparações entre os experimentos.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ka8TyLD0Eun"
   },
   "source": [
    "# 3- BERT para NER\n",
    "\n",
    "Neste tópico, vamos demonstrar como usar a biblioteca Hugging Face Transformers e *pipelines* de um modelo já treinado para realizar *Named Entity Recognition *(NER) em textos em português. Utilizaremos um modelo pré-treinado adequado para a tarefa de NER."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQsqxzNZ2cBq"
   },
   "source": [
    "## 3.1- Configuração de variáveis globais\n",
    "\n",
    "Vamos configurar as variáveis de parâmetros para serem utilizadas durante o código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Tt9jZOW2dUL"
   },
   "outputs": [],
   "source": [
    "# Seleciona a versão do modelo que será utilizada\n",
    "model_id = \"lfcc/bert-portuguese-ner\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QhpLV3pq0_6J"
   },
   "source": [
    "## 3.2- Importação das Bibliotecas\n",
    "\n",
    "Vamos importar as bibliotecas necessárias para carregar o pipeline de NER e o *dataset* em português."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i16vHS_v8KUr"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PedrcaYN1FkY"
   },
   "source": [
    "## 3.3- Carregar o Modelo BERT para NER\n",
    "\n",
    "Vamos carregar o *pipeline* de NER usando um modelo pré-treinado disponível na Hugging Face. Utilizaremos o modelo `xlm-roberta-base`, que é adequado para NER em português.\n",
    "\n",
    "Ao carregar o *pipeline* NER, usando o  aggregation_strategy=\"simple\" o modelo adotará a estratégia de agregaçao para unir tokens que pertençam a uma mesma entidade nomeada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0d7LDxAJ1CzG"
   },
   "outputs": [],
   "source": [
    "# Carregando o modelo e tokenizer\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Carregando o pipeline de NER\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ChW-E1B4299C"
   },
   "source": [
    "## 3.4- Extraindo Entidades de Texto\n",
    "\n",
    "Vamos utilizar o pipeline de NER para identificar entidades nomeadas em alguns exemplos de texto em português."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TdaCWW8M24w7"
   },
   "outputs": [],
   "source": [
    "# Definindo alguns exemplos de texto em português\n",
    "examples = [\n",
    "    \"Paulo viajou para o Estados Unidos.\",\n",
    "    \"Marie Curie foi uma cientista polonesa que realizou pesquisas pioneiras sobre radioatividade.\",\n",
    "    \"Fernando Henrique Cardoso foi o primeiro presidente eleito após a ditadura no Brasil.\",\n",
    "    \"Petrobras foi fundada em 3 de outubro de 1953\"\n",
    "]\n",
    "\n",
    "# Realizando NER nos exemplos de texto\n",
    "for example in examples:\n",
    "    ner_results = ner_pipeline(example)\n",
    "    print(f\"Texto: {example}\")\n",
    "    print(\"Entidades Nomeadas:\")\n",
    "    for entity in ner_results:\n",
    "        print(f\" - {entity['word']}: {entity['entity_group']} ({entity['score']*100:.2f}%)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wFEOSozf75KJ"
   },
   "source": [
    "# 4- **Tradução** Automática usando Hugging Face Transformers\n",
    "\n",
    "Neste tópico, vamos demonstrar como usar a biblioteca Hugging Face Transformers e *pipelines* de um modelo já treinado para realizar a tradução automática de textos em português para o inglês. Utilizaremos um modelo pré-treinado adequado para a tarefa de tradução."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGQoHTzC8CT3"
   },
   "source": [
    "## 4.1- Importação das Bibliotecas\n",
    "\n",
    "Vamos importar as bibliotecas necessárias para carregar o pipeline de tradução."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5IEKNzDL7Vcl"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xEuOo40L8KPH"
   },
   "source": [
    "## 4.2- Carregar o Modelo de Tradução\n",
    "\n",
    "Vamos carregar o *pipeline* de tradução usando um modelo pré-treinado disponível na Hugging Face. Utilizaremos o modelo `Helsinki-NLP/opus-mt-pt-en`, que é adequado para tradução do português para o inglês."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-nsxw_xj8ILi"
   },
   "outputs": [],
   "source": [
    "# Carregando o pipeline de tradução\n",
    "translation_pipeline = pipeline(\"translation_en_to_pt\", model=\"Helsinki-NLP/opus-mt-tc-big-en-pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DYAI2on9HOG"
   },
   "source": [
    "## 4.3- Exemplos de Tradução\n",
    "\n",
    "Vamos utilizar o* pipeline* de tradução para traduzir alguns exemplos de texto em português para o inglês."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4C7IPrgl8Q01"
   },
   "outputs": [],
   "source": [
    "# Definindo alguns exemplos de texto em português\n",
    "examples = [\n",
    "    \"I love learning about natural language processing.\",\n",
    "    \"The BERT model was developed by Google AI Research.\",\n",
    "    \"Machine translation is a challenging and interesting task.\"\n",
    "]\n",
    "\n",
    "# Realizando a tradução dos exemplos de texto\n",
    "for example in examples:\n",
    "    translation = translation_pipeline(example)\n",
    "    print(f\"Texto original: {example}\")\n",
    "    print(f\"Tradução: {translation[0]['translation_text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUU03r1dbyNm"
   },
   "source": [
    "# 4.4- Exercício - Desafio\n",
    "Para finalizar a seção do **BERT** do nosso curso fica o exercício desafio que será divido em três partes:\n",
    "\n",
    "\n",
    "1.   Utilizar o *dataset *`hate-speech-portuguese/hate_speech_portuguese` e dividi-lo em 3 partes `train`, `test` e `val`\n",
    "2.   Usar o *dataset* dividido e avaliar somente a parte do `val`, rodar a predição no modelo `adalbertojunior/distilbert-portuguese-cased` e avaliar a métrica **F1 Score** que deve ser calculada usando o `evaluate` (https://huggingface.co/docs/evaluate/v0.1.2/en/package_reference/loading_methods) do Hugging Face.\n",
    "3.  Realizar um *fine-tunning* para esse *dataset* e avaliar a métrica **F1 Score** comparando. Compare o resuldado desse modelo com o do passo anterior e veja qual ficou melhor.\n",
    "\n",
    "**Importante:**\n",
    "\n",
    "*   Todas as alterações devem ser registradas no Wandb para que seja possível realizar comparações entre os experimentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_udDxu-BbN9N"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
