{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13fe5571",
   "metadata": {},
   "source": [
    "# Guia Prático de Ciência de Dados com MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a937e087",
   "metadata": {},
   "source": [
    "# Aula Prática: Análise e Preparação de Dados\n",
    "\n",
    "Este notebook utiliza um dataset público (Titanic) para demonstrar, passo a passo, os seguintes tópicos:\n",
    "\n",
    "- Análise exploratória e interpretação dos dados\n",
    "- Limpeza e tratamento de dados\n",
    "- Manipulação de dados qualitativos (label encoding, one-hot)\n",
    "- Manipulação de dados quantitativos (normalização, padronização)\n",
    "\n",
    "Ao final, você terá uma base preparada para modelagem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0939b031",
   "metadata": {},
   "source": [
    "\n",
    "## Conteúdos Abordados\n",
    "1. **MLflow**\n",
    "2. **Extração de Dados**\n",
    "3. **Tipos de Dados**\n",
    "4. **Engenharia de Features**\n",
    "5. **Armazenamento e Versionamento de Features com MLflow**\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e441e1b9",
   "metadata": {},
   "source": [
    "## Importando Bibliotecas Necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644e4940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulação e visualização de dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Bibliotecas para aprendizado de máquina\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "\n",
    "# MLflow para rastreamento de experimentos\n",
    "import mlflow\n",
    "\n",
    "# Supressão de avisos\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6341e705",
   "metadata": {},
   "source": [
    "## Noções Básicas sobre o MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d44181c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniciar um experimento no MLflow\n",
    "mlflow.start_run()\n",
    "\n",
    "# Registrar um parâmetro\n",
    "mlflow.log_param(\"parametro1\", \"valor1\")\n",
    "\n",
    "# Registrar uma métrica\n",
    "mlflow.log_metric(\"metrica1\", 0.85)\n",
    "\n",
    "# Encerrar o experimento\n",
    "mlflow.end_run()\n",
    "\n",
    "print(\"Experimento do MLflow registrado com sucesso.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c1acd7",
   "metadata": {},
   "source": [
    "## Extração de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011d79e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo: Importando um dataset do Kaggle\n",
    "# Antes de rodar, certifique-se que a API do Kaggle está configurada e kaggle.json está em ~/.kaggle\n",
    "# Instalar Kaggle caso não esteja instalado: !pip install kaggle\n",
    "\n",
    "# Comando para baixar um dataset do Kaggle\n",
    "# Exemplo: Dataset do Titanic\n",
    "!kaggle competitions download -c titanic\n",
    "\n",
    "# Descompactar o arquivo\n",
    "!unzip titanic.zip -y\n",
    "\n",
    "# Carregar o dataset\n",
    "dados = pd.read_csv(\"train.csv\")\n",
    "dados.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197635c2",
   "metadata": {},
   "source": [
    "## 1) Análise Exploratória e Interpretação dos Dados\n",
    "\n",
    "Nesta seção, investigamos a estrutura, tipos, valores ausentes e distribuições. Também examinamos relações entre variáveis.\n",
    "\n",
    "Passos:\n",
    "- Visualizar amostras (`head`) e dimensões (`shape`)\n",
    "- Tipos e resumo (`info`, `describe`)\n",
    "- Valores ausentes\n",
    "- Distribuições (numéricas e categóricas)\n",
    "- Relações e correlações numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b7ea46",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dados.head())\n",
    "\n",
    "# Exibir informações sobre o dataset\n",
    "print(dados.info())\n",
    "\n",
    "# Exibir dimensões do dataset\n",
    "print('Dimensões:', dados.shape)\n",
    "\n",
    "# Exibir estatísticas básicas\n",
    "display(dados.describe())\n",
    "\n",
    "# Verificar valores ausentes\n",
    "display(dados.isnull().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11781b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribuições numéricas (histogramas)\n",
    "num_cols = dados.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if num_cols:\n",
    "    dados[num_cols].hist(figsize=(12, 8), bins=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Distribuições categóricas (countplot)\n",
    "cat_cols = dados.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()\n",
    "for c in cat_cols[:6]:  # limitar para visualização rápida\n",
    "    plt.figure(figsize=(5,3))\n",
    "    sns.countplot(x=c, data=dados)\n",
    "    plt.title(f'Distribuição de {c}')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Mapa de correlação (apenas colunas numéricas)\n",
    "if len(num_cols) >= 2:\n",
    "    plt.figure(figsize=(8,6))\n",
    "    corr = dados[num_cols].corr()\n",
    "    sns.heatmap(corr, annot=False, cmap='coolwarm', center=0)\n",
    "    plt.title('Correlação entre variáveis numéricas')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b6b402",
   "metadata": {},
   "source": [
    "### Interpretação dos histogramas (variáveis numéricas)\n",
    "\n",
    "- **PassengerId**\n",
    "  - Identificador sequencial; distribuição aproximadamente uniforme por ser apenas um índice. Não tem significado analítico direto e normalmente é descartado como feature.\n",
    "\n",
    "- **Survived**\n",
    "  - Variável-alvo binária (0 = não sobreviveu, 1 = sobreviveu). Observa-se desbalanceamento moderado entre classes, o que pode impactar métricas de modelagem e exigir técnicas de balanceamento.\n",
    "\n",
    "- **Pclass**\n",
    "  - Classe do ticket (1, 2, 3). A maioria dos passageiros viajava na 3ª classe. Essa variável é ordinal e costuma ter forte relação com a sobrevivência (proxy de status socioeconômico).\n",
    "\n",
    "- **Age**\n",
    "  - Distribuição assimétrica à direita, concentrada entre ~20 e ~40 anos, com cauda longa. Possíveis valores ausentes requerem imputação. Crianças e idosos são minoria.\n",
    "\n",
    "- **SibSp** (irmãos/cônjuges a bordo)\n",
    "  - Forte concentração em 0; poucos passageiros tinham muitos acompanhantes nesse grupo. Distribuição altamente assimétrica.\n",
    "\n",
    "- **Parch** (pais/filhos a bordo)\n",
    "  - Também concentrada em 0, com poucos casos maiores que 2. Assimetria semelhante a `SibSp`.\n",
    "\n",
    "- **Fare**\n",
    "  - Altamente assimétrica à direita, com muitos valores baixos e poucos valores muito altos (outliers). Escalonamento e, em alguns casos, transformação logarítmica podem ajudar modelos sensíveis à escala.\n",
    "\n",
    "Observações gerais:\n",
    "- **Assimetria** e **outliers** são comuns em `Fare` e, em menor grau, `Age`.\n",
    "- **Cardinalidade baixa** em `Pclass`, `SibSp` e `Parch` sugere bom potencial para técnicas categóricas/ordinais ou binarizações específicas.\n",
    "- Antes da modelagem, considere: imputação de `Age`, tratamento de outliers em `Fare`, e avaliação de impacto do desbalanceamento de `Survived`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9e498c",
   "metadata": {},
   "source": [
    "### Interpretação do mapa de correlação (variáveis numéricas)\n",
    "\n",
    "- **O que mede**\n",
    "  - Correlação de Pearson no intervalo [-1, 1].\\n  Vermelho = correlação positiva; Azul = negativa; quanto mais intenso, mais forte.\n",
    "\n",
    "- **PassengerId**\n",
    "  - Correlações próximas de 0 com quase todas as variáveis: é um identificador sequencial, sem significado analítico. Normalmente deve ser descartado.\n",
    "\n",
    "- **Survived vs Pclass** (negativa)\n",
    "  - Quanto maior o número da classe (pior a classe), menor a chance de sobrevivência. Sinal coerente com diferenças de acesso a botes/socorro.\n",
    "\n",
    "- **Survived vs Fare** (positiva)\n",
    "  - Passagens mais caras (geralmente classes melhores) associam-se a maior probabilidade de sobrevivência. Relação moderada.\n",
    "\n",
    "- **Survived vs Age** (fraca)\n",
    "  - Relação fraca; idade por si só não explica fortemente o desfecho. Pode haver efeitos não lineares (ex.: crianças).\n",
    "\n",
    "- **Pclass vs Fare** (negativa forte/moderada)\n",
    "  - Multicolinearidade potencial: `Pclass` e `Fare` capturam aspectos semelhantes de status socioeconômico. Cuidado ao usar ambos em modelos sensíveis a colinearidade.\n",
    "\n",
    "- **SibSp vs Parch** (positiva)\n",
    "  - Famílias viajando juntas tendem a ter mais de um desses contadores > 0. Pode indicar redundância parcial entre essas variáveis.\n",
    "\n",
    "- **Demais relações**\n",
    "  - Correlações numéricas em geral são baixas, sugerindo que interações, variáveis categóricas (ex.: `Sex`, `Embarked`) e engenharia de features podem agregar mais sinal.\n",
    "\n",
    "- **Implicações para modelagem**\n",
    "  - Avaliar remover `PassengerId`.\n",
    "  - Monitorar colinearidade entre `Pclass` e `Fare` (regularização, PCA ou seleção de features).\n",
    "  - Considerar variáveis categóricas e interações, além de transformações (ex.: `log(Fare)`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9570e30",
   "metadata": {},
   "source": [
    "## 2) Limpeza e Tratamento de Dados\n",
    "\n",
    "Estratégias comuns:\n",
    "- Remoção de duplicatas\n",
    "- Imputação de valores ausentes em variáveis numéricas (ex.: mediana)\n",
    "- Imputação de valores ausentes em variáveis categóricas (ex.: moda)\n",
    "\n",
    "Abaixo, aplicamos essas estratégias preservando o dataset original em `dados_limpos`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfe2a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_limpos = dados.copy()\n",
    "\n",
    "# Remover duplicatas\n",
    "antes = dados_limpos.shape[0]\n",
    "dados_limpos.drop_duplicates(inplace=True)\n",
    "depois = dados_limpos.shape[0]\n",
    "print(f'Duplicatas removidas: {antes - depois}')\n",
    "\n",
    "# Imputação para colunas numéricas com mediana\n",
    "num_cols = dados_limpos.select_dtypes(include=[np.number]).columns\n",
    "medianas = dados_limpos[num_cols].median()\n",
    "dados_limpos[num_cols] = dados_limpos[num_cols].fillna(medianas)\n",
    "\n",
    "# Imputação para colunas categóricas com moda (valor mais frequente)\n",
    "cat_cols = dados_limpos.select_dtypes(include=['object', 'category', 'bool']).columns\n",
    "for c in cat_cols:\n",
    "    moda = dados_limpos[c].mode(dropna=True)\n",
    "    if not moda.empty:\n",
    "        dados_limpos[c] = dados_limpos[c].fillna(moda.iloc[0])\n",
    "    else:\n",
    "        dados_limpos[c] = dados_limpos[c].fillna('Desconhecido')\n",
    "\n",
    "print('Valores ausentes restantes (depois da imputação):')\n",
    "display(dados_limpos.isnull().sum().sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ec5c98",
   "metadata": {},
   "source": [
    "## 3) Manipulação de Dados Qualitativos (Categóricos)\n",
    "\n",
    "Mostraremos duas abordagens:\n",
    "- Label Encoding (apto para variáveis ordinais ou binárias)\n",
    "- One-Hot Encoding (apto para variáveis nominais com múltiplas categorias)\n",
    "\n",
    "Criaremos duas versões transformadas a partir de `dados_limpos` para fins didáticos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cc732b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_label = dados_limpos.copy()\n",
    "dados_onehot = dados_limpos.copy()\n",
    "\n",
    "# LABEL ENCODING: aplica em colunas categóricas, útil se forem ordinais/binárias\n",
    "le_map = {}\n",
    "cat_cols = dados_label.select_dtypes(include=['object', 'category', 'bool']).columns\n",
    "for c in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    dados_label[c] = le.fit_transform(dados_label[c].astype(str))\n",
    "    le_map[c] = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "\n",
    "print('Exemplo de mapeamento em uma coluna (se existir):')\n",
    "if le_map:\n",
    "    exemplo_col = list(le_map.keys())[0]\n",
    "    print(exemplo_col, '->', list(le_map[exemplo_col])[:5], '...')\n",
    "\n",
    "# ONE-HOT ENCODING: cria colunas dummies para categorias\n",
    "dados_onehot = pd.get_dummies(dados_onehot, drop_first=False, dtype=int)\n",
    "print('Formato após One-Hot:', dados_onehot.shape)\n",
    "display(dados_onehot.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fe1c10",
   "metadata": {},
   "source": [
    "## 4) Manipulação de Dados Quantitativos (Normalização e Padronização)\n",
    "\n",
    "- Normalização (MinMaxScaler): escala valores para um intervalo [0, 1] (ou customizado). Útil quando queremos preservar a forma da distribuição e comparabilidade entre features.\n",
    "- Padronização (StandardScaler): transforma para média 0 e desvio padrão 1. Útil quando suposições de modelos consideram variáveis centradas/escaladas.\n",
    "\n",
    "Aplicaremos as duas técnicas sobre as colunas numéricas de `dados_label` (pois já tratamos os categóricos com Label Encoding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc1a82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_scaled_minmax = dados_label.copy()\n",
    "dados_scaled_standard = dados_label.copy()\n",
    "\n",
    "num_cols = dados_label.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "minmax = MinMaxScaler()\n",
    "dados_scaled_minmax[num_cols] = minmax.fit_transform(dados_scaled_minmax[num_cols])\n",
    "\n",
    "standard = StandardScaler()\n",
    "dados_scaled_standard[num_cols] = standard.fit_transform(dados_scaled_standard[num_cols])\n",
    "\n",
    "print('Visualização após Normalização (Min-Max):')\n",
    "display(dados_scaled_minmax[num_cols].describe().T.head())\n",
    "\n",
    "print('Visualização após Padronização (Standard):')\n",
    "display(dados_scaled_standard[num_cols].describe().T.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fba6d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar um único dataset final e salvar em 'dados_processados.csv'\n",
    "# Estratégia:\n",
    "# - Base: `dados_limpos`\n",
    "# - Categóricas: One-Hot Encoding (sem dropar categoria)\n",
    "# - Numéricas: Padronização (StandardScaler), exceto a coluna-alvo `Survived` (se existir)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "if 'dados_limpos' not in globals():\n",
    "    raise RuntimeError(\"dados_limpos não está disponível. Execute as etapas de limpeza antes desta célula.\")\n",
    "\n",
    "# Identificar colunas de tipos originais (antes do one-hot)\n",
    "cat_cols_orig = dados_limpos.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()\n",
    "num_cols_orig = dados_limpos.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# One-Hot nas categóricas\n",
    "_df = pd.get_dummies(dados_limpos, columns=cat_cols_orig, drop_first=False, dtype=int)\n",
    "\n",
    "# Definir alvo e quais numéricas escalar\n",
    "target_cols = [c for c in ['Survived'] if c in _df.columns]\n",
    "num_to_scale = [c for c in num_cols_orig if c not in target_cols and c in _df.columns]\n",
    "\n",
    "# Padronizar numéricas (mantendo alvo intacto)\n",
    "if num_to_scale:\n",
    "    _scaler_final = StandardScaler()\n",
    "    _df[num_to_scale] = _scaler_final.fit_transform(_df[num_to_scale])\n",
    "\n",
    "# Renomear dataset final\n",
    "dados_processados = _df.copy()\n",
    "\n",
    "# Salvar\n",
    "processed_data_path = \"dados_processados.csv\"\n",
    "dados_processados.to_csv(processed_data_path, index=False)\n",
    "print(f\"dados_processados salvo: {processed_data_path} | shape: {dados_processados.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d2515a",
   "metadata": {},
   "source": [
    "## Armazenamento e Versionamento de Features com MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b795eb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar o dataset processado como um arquivo CSV\n",
    "processed_data_path = \"dados_processados.csv\"\n",
    "dados.to_csv(processed_data_path, index=False)\n",
    "print(\"Dataset processado salvo localmente.\")\n",
    "\n",
    "# Registrar o dataset processado como artefato no MLflow\n",
    "mlflow.start_run()  # Iniciar um novo experimento\n",
    "mlflow.log_artifact(processed_data_path)  # Registrar o arquivo como artefato\n",
    "mlflow.end_run()  # Encerrar o experimento\n",
    "\n",
    "print(\"Features armazenadas e versionadas com sucesso no MLflow!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31350d62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
